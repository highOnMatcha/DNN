{
  "pretrained_models": {
    "gpt2-small": {
      "name": "gpt2",
      "description": "Base GPT-2 (117M parameters) - Good for learning/testing",
      "output_dir": "./models/gpt2_small",
      "from_scratch": false
    },
    "gpt2-medium": {
      "name": "gpt2-medium",
      "description": "Medium GPT-2 (345M parameters) - Better quality",
      "output_dir": "./models/gpt2_medium",
      "from_scratch": false
    },
    "dialogpt-small": {
      "name": "microsoft/DialoGPT-small",
      "description": "DialoGPT Small (117M parameters) - Optimized for dialogue",
      "output_dir": "./models/dialogpt_small",
      "from_scratch": false
    },
    "dialogpt-medium": {
      "name": "microsoft/DialoGPT-medium",
      "description": "DialoGPT Medium (345M parameters) - Better dialogue quality",
      "output_dir": "./models/dialogpt_medium",
      "from_scratch": false
    },
    "alpaca-dialogpt": {
      "name": "AmLibra/dialogpt-small-alpaca",
      "description": "Pre-trained on Alpaca dataset (117M parameters)",
      "output_dir": "./models/alpaca_dialogpt",
      "from_scratch": false
    }
  },
  "custom_models": {
    "custom-small": {
      "name": "custom-gpt-small",
      "description": "Custom GPT model from scratch (50M parameters)",
      "output_dir": "./models/custom_small",
      "from_scratch": true,
      "architecture": {
        "n_embd": 512,
        "n_layer": 8,
        "n_head": 8,
        "vocab_size": 50257,
        "max_sequence_length": 512,
        "dropout": 0.1
      }
    },
    "custom-medium": {
      "name": "custom-gpt-medium",
      "description": "Custom GPT model from scratch (100M parameters)",
      "output_dir": "./models/custom_medium",
      "from_scratch": true,
      "architecture": {
        "n_embd": 768,
        "n_layer": 12,
        "n_head": 12,
        "vocab_size": 50257,
        "max_sequence_length": 512,
        "dropout": 0.1
      }
    },
    "custom-large": {
      "name": "custom-gpt-large",
      "description": "Custom GPT model from scratch (200M parameters)",
      "output_dir": "./models/custom_large",
      "from_scratch": true,
      "architecture": {
        "n_embd": 1024,
        "n_layer": 16,
        "n_head": 16,
        "vocab_size": 50257,
        "max_sequence_length": 512,
        "dropout": 0.1
      }
    },
    "custom-tiny": {
      "name": "custom-gpt-tiny",
      "description": "Tiny custom model for testing (10M parameters)",
      "output_dir": "./models/custom_tiny",
      "from_scratch": true,
      "architecture": {
        "n_embd": 256,
        "n_layer": 4,
        "n_head": 4,
        "vocab_size": 50257,
        "max_sequence_length": 512,
        "dropout": 0.1
      }
    },
    "custom-nano": {
      "name": "custom-gpt-nano",
      "description": "Ultra-tiny model for rapid testing (5M parameters)",
      "output_dir": "./models/custom_nano",
      "from_scratch": true,
      "architecture": {
        "n_embd": 128,
        "n_layer": 3,
        "n_head": 4,
        "vocab_size": 50257,
        "max_sequence_length": 512,
        "dropout": 0.1
      }
    }
  },
  "training_configs": {
    "test": {
      "num_epochs": 1,
      "batch_size": 2,
      "learning_rate": 5e-04,
      "max_samples": 100,
      "description": "Quick testing configuration"
    },
    "development": {
      "num_epochs": 2,
      "batch_size": 4,
      "learning_rate": 3e-04,
      "max_samples": 1000,
      "description": "Development training configuration"
    },
    "production": {
      "num_epochs": 3,
      "batch_size": 8,
      "learning_rate": 1e-04,
      "max_samples": null,
      "description": "Full production training"
    }
  }
}