{
  "pretrained_models": {
    "gpt2-small": {
      "name": "gpt2",
      "description": "Base GPT-2 (117M parameters) - Good for learning/testing",
      "output_dir": "./src/models/gpt2_small",
      "from_scratch": false
    },
    "gpt2-medium": {
      "name": "gpt2-medium",
      "description": "Medium GPT-2 (345M parameters) - Better quality",
      "output_dir": "./src/models/gpt2_medium",
      "from_scratch": false
    },
    "dialogpt-small": {
      "name": "microsoft/DialoGPT-small",
      "description": "DialoGPT Small (117M parameters) - Optimized for dialogue",
      "output_dir": "./src/models/dialogpt_small",
      "from_scratch": false
    },
    "dialogpt-medium": {
      "name": "microsoft/DialoGPT-medium",
      "description": "DialoGPT Medium (345M parameters) - Better dialogue quality",
      "output_dir": "./src/models/dialogpt_medium",
      "from_scratch": false
    },
    "alpaca-dialogpt": {
      "name": "AmLibra/dialogpt-small-alpaca",
      "description": "Pre-trained on Alpaca dataset (117M parameters)",
      "output_dir": "./src/models/alpaca_dialogpt",
      "from_scratch": false
    }
  },
  "custom_models": {
    "custom-small": {
      "name": "custom-gpt-small",
      "description": "Custom GPT model from scratch (50M parameters)",
      "output_dir": "./src/models/custom_small",
      "from_scratch": true,
      "tokenizer_name": "gpt2",
      "architecture": {
        "n_embd": 512,
        "n_layer": 8,
        "n_head": 8,
        "vocab_size": 50257,
        "max_sequence_length": 512,
        "dropout": 0.1
      }
    },
    "custom-medium": {
      "name": "custom-gpt-medium",
      "description": "Custom GPT model from scratch (100M parameters)",
      "output_dir": "./src/models/custom_medium",
      "from_scratch": true,
      "tokenizer_name": "gpt2",
      "architecture": {
        "n_embd": 768,
        "n_layer": 12,
        "n_head": 12,
        "vocab_size": 50257,
        "max_sequence_length": 512,
        "dropout": 0.1
      }
    },
    "custom-large": {
      "name": "custom-gpt-large",
      "description": "Custom GPT model from scratch (200M parameters)",
      "output_dir": "./src/models/custom_large",
      "from_scratch": true,
      "tokenizer_name": "microsoft/DialoGPT-medium",
      "architecture": {
        "n_embd": 1024,
        "n_layer": 16,
        "n_head": 16,
        "vocab_size": 50257,
        "max_sequence_length": 512,
        "dropout": 0.1
      }
    },
    "custom-tiny": {
      "name": "custom-gpt-tiny",
      "description": "Tiny custom model for testing (10M parameters)",
      "output_dir": "./src/models/custom_tiny",
      "from_scratch": true,
      "tokenizer_name": "gpt2",
      "architecture": {
        "n_embd": 256,
        "n_layer": 4,
        "n_head": 4,
        "vocab_size": 50257,
        "max_sequence_length": 512,
        "dropout": 0.1
      }
    },
    "custom-nano": {
      "name": "custom-gpt-nano",
      "description": "Ultra-tiny model for rapid testing (5M parameters)",
      "output_dir": "./src/models/custom_nano",
      "from_scratch": true,
      "tokenizer_name": "gpt2",
      "architecture": {
        "n_embd": 128,
        "n_layer": 3,
        "n_head": 4,
        "vocab_size": 50257,
        "max_sequence_length": 512,
        "dropout": 0.1
      }
    },
    "custom-small-dialogue": {
      "name": "custom-gpt-small-dialogue",
      "description": "Custom small model optimized for dialogue (50M parameters, DialoGPT tokenizer)",
      "output_dir": "./src/models/custom_small_dialogue",
      "from_scratch": true,
      "tokenizer_name": "microsoft/DialoGPT-medium",
      "architecture": {
        "n_embd": 512,
        "n_layer": 8,
        "n_head": 8,
        "vocab_size": 50257,
        "max_sequence_length": 512,
        "dropout": 0.1
      }
    },
    "custom-small-modern": {
      "name": "custom-gpt-small-modern",
      "description": "Custom small model with modern efficient tokenizer (50M parameters, Mistral tokenizer)",
      "output_dir": "./src/models/custom_small_modern",
      "from_scratch": true,
      "tokenizer_name": "mistralai/Mistral-7B-v0.1",
      "architecture": {
        "n_embd": 512,
        "n_layer": 8,
        "n_head": 8,
        "vocab_size": 32000,
        "max_sequence_length": 512,
        "dropout": 0.1
      }
    }
  },
  "training_configs": {
    "test": {
      "num_epochs": 5,
      "batch_size": 4,
      "learning_rate": 5e-04,
      "max_samples": 50,
      "warmup_ratio": 0.1,
      "lr_scheduler_type": "linear",
      "patience": null,
      "early_stopping_threshold": 0.0,
      "weight_decay": 0.01,
      "cache_memory_percent": 0.5,
      "description": "Quick testing - 5 epochs max, linear scheduler, no early stopping for debugging"
    },
    "development": {
      "num_epochs": 15,
      "batch_size": 4,
      "learning_rate": 3e-04,
      "max_samples": 1000,
      "warmup_ratio": 0.15,
      "lr_scheduler_type": "cosine",
      "patience": 3,
      "early_stopping_threshold": 0.005,
      "weight_decay": 0.01,
      "cache_memory_percent": 0.8,
      "description": "Development - up to 15 epochs, cosine scheduler, early stopping after 3 epochs without improvement"
    },
    "production": {
      "num_epochs": 25,
      "batch_size": 8,
      "learning_rate": 1e-04,
      "max_samples": null,
      "warmup_ratio": 0.1,
      "lr_scheduler_type": "cosine",
      "patience": 5,
      "early_stopping_threshold": 0.001,
      "weight_decay": 0.01,
      "cache_memory_percent": 0.9,
      "description": "Production - up to 25 epochs, cosine scheduler, early stopping after 5 epochs without improvement"
    },
    "aggressive": {
      "num_epochs": 50,
      "batch_size": 16,
      "learning_rate": 2e-04,
      "max_samples": null,
      "warmup_ratio": 0.05,
      "lr_scheduler_type": "cosine_with_restarts",
      "patience": 8,
      "early_stopping_threshold": 0.0005,
      "weight_decay": 0.02,
      "cache_memory_percent": 0.95,
      "description": "Aggressive - up to 50 epochs, cosine with restarts, early stopping after 8 epochs for maximum performance"
    },
    "fine_tune": {
      "num_epochs": 10,
      "batch_size": 8,
      "learning_rate": 5e-05,
      "max_samples": null,
      "warmup_ratio": 0.2,
      "lr_scheduler_type": "polynomial",
      "patience": 3,
      "early_stopping_threshold": 0.01,
      "weight_decay": 0.005,
      "cache_memory_percent": 0.8,
      "description": "Fine-tuning - up to 10 epochs (pre-trained models converge faster), early stopping after 3 epochs"
    },
    "long_training": {
      "num_epochs": 100,
      "batch_size": 8,
      "learning_rate": 8e-05,
      "max_samples": null,
      "warmup_ratio": 0.05,
      "lr_scheduler_type": "cosine_with_restarts",
      "patience": 10,
      "early_stopping_threshold": 0.0001,
      "weight_decay": 0.01,
      "cache_memory_percent": 0.9,
      "description": "Long training - up to 100 epochs for complex datasets, very patient early stopping (10 epochs)"
    }
  }
}