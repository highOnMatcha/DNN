{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b61d324",
   "metadata": {},
   "source": [
    "# Model Training and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f4c07ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.append('../')\n",
    "from src.core.trainer import DialogTrainer\n",
    "from src.config.settings import get_model_config, get_test_config, get_development_config\n",
    "from src.data.loaders import get_dataset_manager\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0252f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/khalil/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/khalil/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ WandB connection established\n"
     ]
    }
   ],
   "source": [
    "# Setup WandB authentication\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "wandb_api_key = os.getenv('WANDB_API_KEY')\n",
    "\n",
    "if wandb_api_key:\n",
    "    wandb.login(key=wandb_api_key)\n",
    "    print(\"✓ WandB connection established\")\n",
    "else:\n",
    "    print(\"⚠ No WandB key found - need to fix the .env file\")\n",
    "    print(\"Reminder: Add WANDB_API_KEY=your_actual_key to .env file\")\n",
    "    print(\"Training will continue without WandB logging\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "195f0d1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/khalil/DNN_tmp/llm/notebooks/wandb/run-20250722_172243-tzcy6pu4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tenchishishou-epfl/dialog-model-training/runs/tzcy6pu4' target=\"_blank\">model-comparison-20250722-1722</a></strong> to <a href='https://wandb.ai/tenchishishou-epfl/dialog-model-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tenchishishou-epfl/dialog-model-training' target=\"_blank\">https://wandb.ai/tenchishishou-epfl/dialog-model-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tenchishishou-epfl/dialog-model-training/runs/tzcy6pu4' target=\"_blank\">https://wandb.ai/tenchishishou-epfl/dialog-model-training/runs/tzcy6pu4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Main WandB experiment tracking started\n",
      "Run name: model-comparison-20250722-1722\n",
      "Project: dialog-model-training\n"
     ]
    }
   ],
   "source": [
    "# Initialize main WandB run for model comparison experiment\n",
    "main_run = wandb.init(\n",
    "    project=\"dialog-model-training\",\n",
    "    name=f\"model-comparison-{datetime.now().strftime('%Y%m%d-%H%M')}\",\n",
    "    tags=[\"comparison\", \"training\", \"notebook\"],\n",
    "    notes=\"Comparing multiple dialog model architectures: GPT-2, DialoGPT, and custom models\"\n",
    ")\n",
    "\n",
    "print(\"✓ Main WandB experiment tracking started\")\n",
    "print(f\"Run name: {main_run.name}\")\n",
    "print(f\"Project: {main_run.project}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2642a72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from local file: data/alpaca-gpt4.csv\n",
      "Training Configuration:\n",
      "- Epochs: 1\n",
      "- Batch size: 2\n",
      "- Max samples: 100\n",
      "- Learning rate: 5e-05\n",
      "\n",
      "Models to train and compare: ['custom-small']\n",
      "Dataset loaded: 52002 samples\n",
      "Training Configuration:\n",
      "- Epochs: 1\n",
      "- Batch size: 2\n",
      "- Max samples: 100\n",
      "- Learning rate: 5e-05\n",
      "\n",
      "Models to train and compare: ['custom-small']\n",
      "Dataset loaded: 52002 samples\n"
     ]
    }
   ],
   "source": [
    "# Load dataset and configuration\n",
    "dataset_manager = get_dataset_manager()\n",
    "df = dataset_manager.load_dataset()\n",
    "training_config = get_development_config()\n",
    "\n",
    "# Models to compare\n",
    "models_to_train = [\n",
    "    #\"gpt2-small\",      # Pre-trained GPT-2\n",
    "    \"custom-small\",    # Custom model from scratch\n",
    "    #\"dialogpt-small\"   # Pre-trained DialoGPT\n",
    "]\n",
    "\n",
    "# Single consolidated output to avoid duplicates\n",
    "output = []\n",
    "output.append(\"Training Configuration:\")\n",
    "output.append(f\"- Epochs: {training_config.num_epochs}\")\n",
    "output.append(f\"- Batch size: {training_config.batch_size}\")\n",
    "output.append(f\"- Max samples: {training_config.max_samples}\")\n",
    "output.append(f\"- Learning rate: {training_config.learning_rate}\")\n",
    "output.append(f\"\\nModels to train and compare: {models_to_train}\")\n",
    "output.append(f\"Dataset loaded: {len(df)} samples\")\n",
    "\n",
    "print(\"\\n\".join(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b50488b",
   "metadata": {},
   "source": [
    "## Training Loop with WandB Tracking\n",
    "\n",
    "Train each model variant and log metrics to WandB for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1aab7902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_logging(model_name, df, training_config):\n",
    "    \"\"\"Train a model with WandB logging\"\"\"\n",
    "    \n",
    "    # Initialize WandB run for this model (separate project to avoid conflicts)\n",
    "    run = wandb.init(\n",
    "        project=\"dialog-model-training-individual\",\n",
    "        name=f\"training-{model_name}-{datetime.now().strftime('%Y%m%d-%H%M')}\",\n",
    "        tags=[\"training\", model_name],\n",
    "        config={\n",
    "            \"model\": model_name,\n",
    "            \"epochs\": training_config.num_epochs,\n",
    "            \"batch_size\": training_config.batch_size,\n",
    "            \"learning_rate\": training_config.learning_rate,\n",
    "            \"max_samples\": training_config.max_samples\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Get model config and initialize trainer\n",
    "        model_config = get_model_config(model_name)\n",
    "        trainer = DialogTrainer(model_config=model_config)\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training {model_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Prepare dataset\n",
    "        train_dataset, eval_dataset = trainer.prepare_dataset(\n",
    "            df, \n",
    "            max_samples=training_config.max_samples\n",
    "        )\n",
    "        \n",
    "        # Start training\n",
    "        start_time = datetime.now()\n",
    "        trainer.train(\n",
    "            train_dataset, \n",
    "            eval_dataset, \n",
    "            num_epochs=training_config.num_epochs,\n",
    "            batch_size=training_config.batch_size,\n",
    "            learning_rate=training_config.learning_rate\n",
    "        )\n",
    "        end_time = datetime.now()\n",
    "        \n",
    "        training_time = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        # Log final metrics\n",
    "        wandb.log({\n",
    "            \"training_time_seconds\": training_time,\n",
    "            \"training_time_minutes\": training_time / 60,\n",
    "            \"model_parameters\": sum(p.numel() for p in trainer.model.parameters()),\n",
    "            \"dataset_size\": len(train_dataset),\n",
    "            \"eval_size\": len(eval_dataset)\n",
    "        })\n",
    "        \n",
    "        print(f\"Training completed in {training_time/60:.1f} minutes\")\n",
    "        return trainer\n",
    "        \n",
    "    finally:\n",
    "        wandb.finish()\n",
    "\n",
    "# Store trained models for comparison\n",
    "trained_models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "031a2f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing initial WandB run...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">model-comparison-20250722-1722</strong> at: <a href='https://wandb.ai/tenchishishou-epfl/dialog-model-training/runs/tzcy6pu4' target=\"_blank\">https://wandb.ai/tenchishishou-epfl/dialog-model-training/runs/tzcy6pu4</a><br> View project at: <a href='https://wandb.ai/tenchishishou-epfl/dialog-model-training' target=\"_blank\">https://wandb.ai/tenchishishou-epfl/dialog-model-training</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250722_172243-tzcy6pu4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training for custom-small...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/khalil/DNN_tmp/llm/notebooks/wandb/run-20250722_172246-14mlqcvi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tenchishishou-epfl/dialog-model-training-individual/runs/14mlqcvi' target=\"_blank\">training-custom-small-20250722-1722</a></strong> to <a href='https://wandb.ai/tenchishishou-epfl/dialog-model-training-individual' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tenchishishou-epfl/dialog-model-training-individual' target=\"_blank\">https://wandb.ai/tenchishishou-epfl/dialog-model-training-individual</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tenchishishou-epfl/dialog-model-training-individual/runs/14mlqcvi' target=\"_blank\">https://wandb.ai/tenchishishou-epfl/dialog-model-training-individual/runs/14mlqcvi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building custom model from scratch: custom-gpt-small\n",
      "Architecture: 8 layers, 512 dim, 8 heads\n",
      "Output directory: ./models/custom_small\n",
      "Custom GPT model initialized:\n",
      "  Parameters: 76,945,408\n",
      "  Layers: 8\n",
      "  Embedding dim: 512\n",
      "  Attention heads: 8\n",
      "Device: cpu\n",
      "Model parameters: 76,945,408\n",
      "\n",
      "==================================================\n",
      "Training custom-small\n",
      "==================================================\n",
      "Using 100 samples for training\n",
      "Training samples: 90\n",
      "Evaluation samples: 10\n",
      "Custom GPT model initialized:\n",
      "  Parameters: 76,945,408\n",
      "  Layers: 8\n",
      "  Embedding dim: 512\n",
      "  Attention heads: 8\n",
      "Device: cpu\n",
      "Model parameters: 76,945,408\n",
      "\n",
      "==================================================\n",
      "Training custom-small\n",
      "==================================================\n",
      "Using 100 samples for training\n",
      "Training samples: 90\n",
      "Evaluation samples: 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e08de97b9e4f66881f79b04f0e2c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/90 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f5a394cc3284c3ebefa58831e8a9ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/khalil/DNN_tmp/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 00:49, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed! Model saved to ./models/custom_small\n",
      "Training completed in 0.8 minutes\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>dataset_size</td><td>▁</td></tr><tr><td>eval_size</td><td>▁</td></tr><tr><td>model_parameters</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>training_time_minutes</td><td>▁</td></tr><tr><td>training_time_seconds</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>dataset_size</td><td>90</td></tr><tr><td>eval_size</td><td>10</td></tr><tr><td>model_parameters</td><td>76945408</td></tr><tr><td>total_flos</td><td>6872362598400.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>45</td></tr><tr><td>train_loss</td><td>10.15426</td></tr><tr><td>train_runtime</td><td>50.1872</td></tr><tr><td>train_samples_per_second</td><td>1.793</td></tr><tr><td>train_steps_per_second</td><td>0.897</td></tr><tr><td>training_time_minutes</td><td>0.84981</td></tr><tr><td>training_time_seconds</td><td>50.98875</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">training-custom-small-20250722-1722</strong> at: <a href='https://wandb.ai/tenchishishou-epfl/dialog-model-training-individual/runs/14mlqcvi' target=\"_blank\">https://wandb.ai/tenchishishou-epfl/dialog-model-training-individual/runs/14mlqcvi</a><br> View project at: <a href='https://wandb.ai/tenchishishou-epfl/dialog-model-training-individual' target=\"_blank\">https://wandb.ai/tenchishishou-epfl/dialog-model-training-individual</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250722_172246-14mlqcvi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESSFULLY trained custom-small\n",
      "\n",
      "Training Summary:\n",
      "Successfully trained: ['custom-small']\n",
      "Total models trained: 1\n",
      "\n",
      "Initializing main WandB run for generation testing...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.2s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/khalil/DNN_tmp/llm/notebooks/wandb/run-20250722_172342-gpulq253</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tenchishishou-epfl/dialog-model-training/runs/gpulq253' target=\"_blank\">generation-results-20250722-1723</a></strong> to <a href='https://wandb.ai/tenchishishou-epfl/dialog-model-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tenchishishou-epfl/dialog-model-training' target=\"_blank\">https://wandb.ai/tenchishishou-epfl/dialog-model-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tenchishishou-epfl/dialog-model-training/runs/gpulq253' target=\"_blank\">https://wandb.ai/tenchishishou-epfl/dialog-model-training/runs/gpulq253</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Close initial WandB run before starting individual training\n",
    "print(\"Closing initial WandB run...\")\n",
    "main_run.finish()\n",
    "\n",
    "# Train all models\n",
    "for model_name in models_to_train:\n",
    "    print(f\"\\nStarting training for {model_name}...\")\n",
    "    try:\n",
    "        trainer = train_model_with_logging(model_name, df, training_config)\n",
    "        trained_models[model_name] = trainer\n",
    "        print(f\"SUCCESSFULLY trained {model_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"FAILED to train {model_name}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nTraining Summary:\")\n",
    "print(f\"Successfully trained: {list(trained_models.keys())}\")\n",
    "print(f\"Total models trained: {len(trained_models)}\")\n",
    "\n",
    "# Initialize main WandB run for logging generation results\n",
    "print(\"\\nInitializing main WandB run for generation testing...\")\n",
    "main_run = wandb.init(\n",
    "    project=\"dialog-model-training\",\n",
    "    name=f\"generation-results-{datetime.now().strftime('%Y%m%d-%H%M')}\",\n",
    "    tags=[\"comparison\", \"generation\", \"notebook\"],\n",
    "    notes=\"Generation testing results for trained dialog models\",\n",
    "    config={\n",
    "        \"dataset_size\": len(df),\n",
    "        \"models_trained\": list(trained_models.keys()),\n",
    "        \"num_models\": len(trained_models)\n",
    "    }\n",
    ")\n",
    "\n",
    "# Log training completion summary to main run\n",
    "wandb.log({\n",
    "    \"training_completed\": True,\n",
    "    \"models_successfully_trained\": len(trained_models),\n",
    "    \"training_completion_time\": datetime.now().timestamp()\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229cef38",
   "metadata": {},
   "source": [
    "## Dialog Generation Testing\n",
    "\n",
    "Test the trained models by generating responses to sample instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8d568af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialog Generation Comparison\n",
      "============================================================\n",
      "\n",
      "[1/5] Instruction: Explain what machine learning is in simple terms.\n",
      "----------------------------------------\n",
      "\n",
      "custom-small: bill\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      " the.\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " the\n",
      " options the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      " the\n",
      "\n",
      "\n",
      "\n",
      " the\n",
      "\n",
      "\n",
      " task the\n",
      ". Store,bill such Quan\n",
      " the\n",
      "\n",
      "\n",
      " helmet\n",
      ",. the\n",
      ", a,\n",
      "\n",
      "xton.,\n",
      "\n",
      " the\n",
      "\n",
      "\n",
      ".\n",
      ",,\n",
      " Quan\n",
      "============================================================\n",
      "\n",
      "[2/5] Instruction: How do I make a good cup of coffee?\n",
      "----------------------------------------\n",
      "\n",
      "custom-small: bill\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      " the.\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " the\n",
      " options the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      " the\n",
      "\n",
      "\n",
      "\n",
      " the\n",
      "\n",
      "\n",
      " task the\n",
      ". Store,bill such Quan\n",
      " the\n",
      "\n",
      "\n",
      " helmet\n",
      ",. the\n",
      ", a,\n",
      "\n",
      "xton.,\n",
      "\n",
      " the\n",
      "\n",
      "\n",
      ".\n",
      ",,\n",
      " Quan\n",
      "============================================================\n",
      "\n",
      "[2/5] Instruction: How do I make a good cup of coffee?\n",
      "----------------------------------------\n",
      "\n",
      "custom-small: the.\n",
      "\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      " the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " the\n",
      "\n",
      "\n",
      "bill\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " reliable\n",
      " the Thirty thexton\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " options925\n",
      " final and\n",
      "\n",
      " the such the\n",
      "\n",
      "\n",
      " Stanford.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      ",\n",
      ",\n",
      "\n",
      "\n",
      " Columbia,.dm.\n",
      "\n",
      ",\n",
      " the horse and\n",
      " horse. legislative\n",
      "============================================================\n",
      "\n",
      "[3/5] Instruction: What are the benefits of exercise?\n",
      "----------------------------------------\n",
      "\n",
      "custom-small: the.\n",
      "\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      " the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " the\n",
      "\n",
      "\n",
      "bill\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " reliable\n",
      " the Thirty thexton\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " options925\n",
      " final and\n",
      "\n",
      " the such the\n",
      "\n",
      "\n",
      " Stanford.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      ",\n",
      ",\n",
      "\n",
      "\n",
      " Columbia,.dm.\n",
      "\n",
      ",\n",
      " the horse and\n",
      " horse. legislative\n",
      "============================================================\n",
      "\n",
      "[3/5] Instruction: What are the benefits of exercise?\n",
      "----------------------------------------\n",
      "\n",
      "custom-small: the.\n",
      "###\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " the.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      " theYesterday\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      " in,Yesterday. the\n",
      " the FILE\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ". horse\n",
      ".. a.:. the\n",
      "\n",
      "arist.\n",
      "\n",
      "\n",
      "\n",
      " a\n",
      ",.,\n",
      ".\n",
      "\n",
      ".\n",
      "============================================================\n",
      "\n",
      "[4/5] Instruction: Tell me about the solar system.\n",
      "----------------------------------------\n",
      "\n",
      "custom-small: the.\n",
      "###\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " the.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      " theYesterday\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      " in,Yesterday. the\n",
      " the FILE\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ". horse\n",
      ".. a.:. the\n",
      "\n",
      "arist.\n",
      "\n",
      "\n",
      "\n",
      " a\n",
      ",.,\n",
      ".\n",
      "\n",
      ".\n",
      "============================================================\n",
      "\n",
      "[4/5] Instruction: Tell me about the solar system.\n",
      "----------------------------------------\n",
      "\n",
      "custom-small: ..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Streamer\n",
      "\n",
      ", integration\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",.\n",
      "\n",
      ",,\n",
      "\n",
      " theeq\n",
      " the,.\n",
      "\n",
      "\n",
      " reliable\n",
      " the\n",
      "dm\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      ".\n",
      "925 Columbia\n",
      " the\n",
      ",\n",
      ".\n",
      "..\n",
      ":.,\n",
      ".\n",
      "============================================================\n",
      "\n",
      "[5/5] Instruction: How can I improve my communication skills?\n",
      "----------------------------------------\n",
      "\n",
      "custom-small: ..\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Streamer\n",
      "\n",
      ", integration\n",
      ":\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",.\n",
      "\n",
      ",,\n",
      "\n",
      " theeq\n",
      " the,.\n",
      "\n",
      "\n",
      " reliable\n",
      " the\n",
      "dm\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      ".\n",
      "925 Columbia\n",
      " the\n",
      ",\n",
      ".\n",
      "..\n",
      ":.,\n",
      ".\n",
      "============================================================\n",
      "\n",
      "[5/5] Instruction: How can I improve my communication skills?\n",
      "----------------------------------------\n",
      "\n",
      "custom-small: ,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      " the incident\n",
      "\n",
      "\n",
      " maintaining\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      " the the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      " is\n",
      "\n",
      " the.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " the\n",
      "\n",
      "\n",
      " the\n",
      "\n",
      " STD\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      " bite\n",
      "dm the,.\n",
      " the\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      " a\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      " such,\n",
      "\n",
      " tomatoes.\n",
      "xton,\n",
      "============================================================\n",
      "Generation testing completed!\n",
      "Generated 5 total responses\n",
      "\n",
      "custom-small: ,\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      " the incident\n",
      "\n",
      "\n",
      " maintaining\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      " the the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      " is\n",
      "\n",
      " the.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " the\n",
      "\n",
      "\n",
      " the\n",
      "\n",
      " STD\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ",\n",
      "\n",
      "\n",
      "\n",
      " bite\n",
      "dm the,.\n",
      " the\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      " a\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      " such,\n",
      "\n",
      " tomatoes.\n",
      "xton,\n",
      "============================================================\n",
      "Generation testing completed!\n",
      "Generated 5 total responses\n"
     ]
    }
   ],
   "source": [
    "# Test instructions for dialog generation\n",
    "test_instructions = [\n",
    "    \"Explain what machine learning is in simple terms.\",\n",
    "    \"How do I make a good cup of coffee?\",\n",
    "    \"What are the benefits of exercise?\",\n",
    "    \"Tell me about the solar system.\",\n",
    "    \"How can I improve my communication skills?\"\n",
    "]\n",
    "\n",
    "print(\"Dialog Generation Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Store all responses for WandB logging\n",
    "all_responses = []\n",
    "generation_metrics = {}\n",
    "\n",
    "for i, instruction in enumerate(test_instructions, 1):\n",
    "    print(f\"\\n[{i}/{len(test_instructions)}] Instruction: {instruction}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    instruction_responses = {\"instruction\": instruction, \"responses\": {}}\n",
    "    \n",
    "    for model_name, trainer in trained_models.items():\n",
    "        try:\n",
    "            start_time = datetime.now()\n",
    "            response = trainer.generate_response(instruction, max_length=100)\n",
    "            generation_time = (datetime.now() - start_time).total_seconds()\n",
    "            \n",
    "            print(f\"\\n{model_name}: {response}\")\n",
    "            \n",
    "            # Store for logging\n",
    "            instruction_responses[\"responses\"][model_name] = {\n",
    "                \"text\": response,\n",
    "                \"generation_time\": generation_time,\n",
    "                \"length\": len(response),\n",
    "                \"word_count\": len(response.split())\n",
    "            }\n",
    "            \n",
    "            # Track metrics per model\n",
    "            if model_name not in generation_metrics:\n",
    "                generation_metrics[model_name] = {\n",
    "                    \"total_time\": 0,\n",
    "                    \"total_responses\": 0,\n",
    "                    \"total_length\": 0,\n",
    "                    \"total_words\": 0\n",
    "                }\n",
    "            \n",
    "            generation_metrics[model_name][\"total_time\"] += generation_time\n",
    "            generation_metrics[model_name][\"total_responses\"] += 1\n",
    "            generation_metrics[model_name][\"total_length\"] += len(response)\n",
    "            generation_metrics[model_name][\"total_words\"] += len(response.split())\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n{model_name}: Error - {e}\")\n",
    "            instruction_responses[\"responses\"][model_name] = {\n",
    "                \"error\": str(e),\n",
    "                \"generation_time\": 0,\n",
    "                \"length\": 0,\n",
    "                \"word_count\": 0\n",
    "            }\n",
    "    \n",
    "    all_responses.append(instruction_responses)\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "print(f\"Generation testing completed!\")\n",
    "print(f\"Generated {sum(len(r['responses']) for r in all_responses if 'responses' in r)} total responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f446b1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging generation metrics to WandB...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>custom-small_avg_generation_time</td><td>▁</td></tr><tr><td>custom-small_avg_response_length</td><td>▁</td></tr><tr><td>custom-small_avg_response_words</td><td>▁</td></tr><tr><td>custom-small_responses_generated</td><td>▁</td></tr><tr><td>custom-small_total_generation_time</td><td>▁</td></tr><tr><td>models_successfully_trained</td><td>▁</td></tr><tr><td>models_trained</td><td>▁</td></tr><tr><td>total_generations</td><td>▁</td></tr><tr><td>total_test_instructions</td><td>▁</td></tr><tr><td>training_completion_time</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>custom-small_avg_generation_time</td><td>7.95595</td></tr><tr><td>custom-small_avg_response_length</td><td>165.2</td></tr><tr><td>custom-small_avg_response_words</td><td>26.2</td></tr><tr><td>custom-small_responses_generated</td><td>5</td></tr><tr><td>custom-small_total_generation_time</td><td>39.77974</td></tr><tr><td>experiment_status</td><td>completed</td></tr><tr><td>generation_test_completed</td><td>True</td></tr><tr><td>models_successfully_trained</td><td>1</td></tr><tr><td>models_trained</td><td>1</td></tr><tr><td>total_generations</td><td>5</td></tr><tr><td>total_test_instructions</td><td>5</td></tr><tr><td>training_completed</td><td>True</td></tr><tr><td>training_completion_time</td><td>1753197824.19334</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">generation-results-20250722-1723</strong> at: <a href='https://wandb.ai/tenchishishou-epfl/dialog-model-training/runs/gpulq253' target=\"_blank\">https://wandb.ai/tenchishishou-epfl/dialog-model-training/runs/gpulq253</a><br> View project at: <a href='https://wandb.ai/tenchishishou-epfl/dialog-model-training' target=\"_blank\">https://wandb.ai/tenchishishou-epfl/dialog-model-training</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250722_172342-gpulq253/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment completed successfully.\n",
      "Trained 1 models and completed generation testing.\n"
     ]
    }
   ],
   "source": [
    "# Log generation metrics to main WandB run\n",
    "print(\"Logging generation metrics to WandB...\")\n",
    "\n",
    "# Log average metrics per model\n",
    "for model_name, metrics in generation_metrics.items():\n",
    "    if metrics[\"total_responses\"] > 0:  # Avoid division by zero\n",
    "        avg_metrics = {\n",
    "            f\"{model_name}_avg_generation_time\": metrics[\"total_time\"] / metrics[\"total_responses\"],\n",
    "            f\"{model_name}_avg_response_length\": metrics[\"total_length\"] / metrics[\"total_responses\"],\n",
    "            f\"{model_name}_avg_response_words\": metrics[\"total_words\"] / metrics[\"total_responses\"],\n",
    "            f\"{model_name}_total_generation_time\": metrics[\"total_time\"],\n",
    "            f\"{model_name}_responses_generated\": metrics[\"total_responses\"]\n",
    "        }\n",
    "        wandb.log(avg_metrics)\n",
    "\n",
    "# Create a summary table for WandB\n",
    "response_table = wandb.Table(columns=[\"Model\", \"Instruction\", \"Response\", \"Length\", \"Words\", \"Time (s)\"])\n",
    "\n",
    "for response_set in all_responses:\n",
    "    instruction = response_set[\"instruction\"]\n",
    "    for model_name, response_data in response_set[\"responses\"].items():\n",
    "        if \"error\" not in response_data:\n",
    "            response_table.add_data(\n",
    "                model_name,\n",
    "                instruction[:50] + \"...\" if len(instruction) > 50 else instruction,\n",
    "                response_data[\"text\"][:100] + \"...\" if len(response_data[\"text\"]) > 100 else response_data[\"text\"],\n",
    "                response_data[\"length\"],\n",
    "                response_data[\"word_count\"],\n",
    "                round(response_data[\"generation_time\"], 3)\n",
    "            )\n",
    "\n",
    "wandb.log({\"generation_comparison\": response_table})\n",
    "\n",
    "# Log overall generation stats and final summary\n",
    "final_summary = {\n",
    "    \"experiment_status\": \"completed\",\n",
    "    \"models_trained\": len(trained_models),\n",
    "    \"total_test_instructions\": len(test_instructions),\n",
    "    \"total_generations\": sum(len(r[\"responses\"]) for r in all_responses if \"responses\" in r),\n",
    "    \"generation_test_completed\": True\n",
    "}\n",
    "\n",
    "wandb.log(final_summary)\n",
    "\n",
    "# Close WandB run\n",
    "main_run.finish()\n",
    "\n",
    "print(\"Experiment completed successfully.\")\n",
    "print(f\"Trained {len(trained_models)} models and completed generation testing.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
