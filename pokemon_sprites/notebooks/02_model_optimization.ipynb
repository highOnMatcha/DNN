{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "891e421c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment ready\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Project paths\n",
    "src_path = Path(\"../src\")\n",
    "data_dir = Path(\"../data\")\n",
    "sys.path.insert(0, str(src_path))\n",
    "\n",
    "print(\"Environment ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0079f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Curated Model Architecture Analysis\n",
      "==================================================\n",
      "Focus: Top 3 models for sprite generation task\n",
      "\n",
      "Analyzing 3 selected models\n",
      "\n",
      "LIGHTWEIGHT BASELINE:\n",
      "  Purpose: Lightweight baseline for quick experimentation - Fast training with minimal parameters\n",
      "  Architecture: pix2pix\n",
      "  Generator:\n",
      "    Channels: 3 -> 4\n",
      "    Base features: 32\n",
      "    Residual blocks: 6\n",
      "    Normalization: instance\n",
      "    Dropout: 0.3\n",
      "  Discriminator:\n",
      "    Input channels: 7 (artwork+sprite)\n",
      "    Base features: 32\n",
      "    Layers: 2\n",
      "  Training:\n",
      "    Image size: 256px\n",
      "    L1 loss weight: 150.0\n",
      "  Estimated parameters: ~0.0M\n",
      "  Strengths:\n",
      "    + Fast training and inference\n",
      "    + Low memory requirements\n",
      "    + Reduced overfitting risk\n",
      "    + Direct 256px compatibility\n",
      "    + RGBA transparency support\n",
      "    + Strong pixel-level accuracy emphasis\n",
      "  Considerations:\n",
      "    - May lack capacity for complex mappings\n",
      "  Suitability Score: 10/12\n",
      "\n",
      "SPRITE OPTIMIZED:\n",
      "  Purpose: State-of-the-art configuration optimized specifically for pixel art sprite generation\n",
      "  Architecture: pix2pix\n",
      "  Generator:\n",
      "    Channels: 3 -> 4\n",
      "    Base features: 64\n",
      "    Residual blocks: 9\n",
      "    Normalization: instance\n",
      "    Dropout: 0.3\n",
      "    Features: Self-attention mechanism\n",
      "  Discriminator:\n",
      "    Input channels: 7 (artwork+sprite)\n",
      "    Base features: 64\n",
      "    Layers: 3\n",
      "    Features: Spectral normalization\n",
      "  Training:\n",
      "    Image size: 256px\n",
      "    L1 loss weight: 150.0\n",
      "    Perceptual loss: 0.1\n",
      "    Pixel art loss: 10.0\n",
      "  Estimated parameters: ~0.1M\n",
      "  Strengths:\n",
      "    + Attention mechanism for detail preservation\n",
      "    + Optimized loss weights for pixel art\n",
      "    + Spectral normalization for stability\n",
      "    + RGBA support with proper channel handling\n",
      "    + Direct 256px compatibility\n",
      "    + RGBA transparency support\n",
      "    + Strong pixel-level accuracy emphasis\n",
      "  Considerations:\n",
      "    - Balanced complexity for dataset size\n",
      "  Suitability Score: 12/12\n",
      "\n",
      "TRANSFORMER ENHANCED:\n",
      "  Purpose: Advanced transformer-enhanced architecture for complex artwork-to-sprite mappings\n",
      "  Architecture: pix2pix_transformer\n",
      "  Generator:\n",
      "    Channels: 3 -> 4\n",
      "    Base features: 64\n",
      "    Residual blocks: 8\n",
      "    Normalization: instance\n",
      "    Dropout: 0.2\n",
      "    Features: 4 transformer layers\n",
      "    Features: 8 attention heads\n",
      "  Discriminator:\n",
      "    Input channels: 7 (artwork+sprite)\n",
      "    Base features: 64\n",
      "    Layers: 3\n",
      "    Features: Spectral normalization\n",
      "  Training:\n",
      "    Image size: 256px\n",
      "    L1 loss weight: 150.0\n",
      "    Perceptual loss: 0.15\n",
      "  Estimated parameters: ~0.6M\n",
      "  Strengths:\n",
      "    + Long-range dependency modeling\n",
      "    + Advanced attention mechanisms\n",
      "    + State-of-the-art architecture\n",
      "    + Direct 256px compatibility\n",
      "    + RGBA transparency support\n",
      "    + Strong pixel-level accuracy emphasis\n",
      "  Considerations:\n",
      "    - Higher computational requirements\n",
      "    - May need careful regularization\n",
      "  Suitability Score: 11/12\n",
      "\n",
      "--------------------------------------------------\n",
      "IMPLEMENTATION STRATEGY\n",
      "--------------------------------------------------\n",
      "\n",
      "Recommended Training Sequence:\n",
      "  1. Sprite Optimized\n",
      "     Role: PRIMARY - Main production model\n",
      "     Score: 12/12\n",
      "     Use case: Start here for quick results\n",
      "  2. Transformer Enhanced\n",
      "     Role: ADVANCED - Experimental state-of-the-art\n",
      "     Score: 11/12\n",
      "     Use case: Optimize after baseline\n",
      "  3. Lightweight Baseline\n",
      "     Role: BASELINE - Quick validation and debugging\n",
      "     Score: 10/12\n",
      "     Use case: Experiment if resources allow\n",
      "\n",
      "Curriculum Learning Compatibility:\n",
      "  All models support:\n",
      "    • Progressive input scaling (128px -> 192px -> 256px)\n",
      "    • Fixed 256px sprite output\n",
      "    • RGBA channel handling\n",
      "    • Instance normalization (better for style transfer)\n",
      "    • Optimized loss weights for pixel art\n",
      "\n",
      "Next Steps:\n",
      "  1. Start with lightweight-baseline for rapid prototyping\n",
      "  2. Validate data pipeline and training stability\n",
      "  3. Switch to sprite-optimized for production training\n",
      "  4. Experiment with transformer-enhanced if results plateau\n",
      "  5. Apply curriculum learning with progressively larger inputs\n",
      "\n",
      "SUMMARY:\n",
      "==================================================\n",
      "Configuration cleaned up from 8+ models to 3 focused architectures:\n",
      "  • Removed outdated/suboptimal configurations\n",
      "  • Kept only models with 256px support and RGBA channels\n",
      "  • Focused on curriculum learning compatibility\n",
      "  • Ready for immediate implementation\n"
     ]
    }
   ],
   "source": [
    "def analyze_model_architectures():\n",
    "    \"\"\"\n",
    "    Analyze the 3 curated model architectures for Pokemon sprite generation.\n",
    "    \n",
    "    Evaluates our streamlined selection: lightweight baseline, sprite-optimized, \n",
    "    and transformer-enhanced models for artwork-to-sprite translation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load model configurations from repository\n",
    "    config_path = src_path / \"config\" / \"model_configs.json\"\n",
    "    \n",
    "    if not config_path.exists():\n",
    "        print(\"Model configuration file not available\")\n",
    "        return\n",
    "    \n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    print(\"Curated Model Architecture Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Focus: Top 3 models for sprite generation task\")\n",
    "    \n",
    "    # Analyze available model configurations\n",
    "    model_configs = config.get('pix2pix_models', {})\n",
    "    \n",
    "    if not model_configs:\n",
    "        print(\"No model configurations found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nAnalyzing {len(model_configs)} selected models\")\n",
    "    \n",
    "    architecture_analysis = {}\n",
    "    \n",
    "    for model_name, model_config in model_configs.items():\n",
    "        print(f\"\\n{model_name.upper().replace('-', ' ')}:\")\n",
    "        print(f\"  Purpose: {model_config.get('description', 'No description')}\")\n",
    "        \n",
    "        params = model_config.get('parameters', {})\n",
    "        generator = params.get('generator', {})\n",
    "        discriminator = params.get('discriminator', {})\n",
    "        \n",
    "        # More accurate parameter estimation\n",
    "        ngf = generator.get('ngf', 64)\n",
    "        ndf = discriminator.get('ndf', 64)\n",
    "        n_blocks = generator.get('n_blocks', 9)\n",
    "        d_layers = discriminator.get('n_layers', 3)\n",
    "        \n",
    "        # Parameter calculation for different architectures\n",
    "        if 'transformer' in model_name:\n",
    "            # Transformer model has additional parameters\n",
    "            transformer_layers = generator.get('transformer_layers', 4)\n",
    "            attention_heads = generator.get('attention_heads', 8)\n",
    "            transformer_params = transformer_layers * attention_heads * ngf * ngf * 4\n",
    "            base_params = ngf * ngf * (n_blocks * 2 + 8) + ndf * ndf * (d_layers + 2)\n",
    "            total_params = (base_params + transformer_params) / 1000000\n",
    "        else:\n",
    "            # Standard pix2pix calculation\n",
    "            gen_params = ngf * ngf * (n_blocks * 2 + 10)\n",
    "            disc_params = ndf * ndf * (d_layers + 2)\n",
    "            total_params = (gen_params + disc_params) / 1000000\n",
    "        \n",
    "        print(f\"  Architecture: {model_config.get('architecture', 'pix2pix')}\")\n",
    "        print(f\"  Generator:\")\n",
    "        print(f\"    Channels: {generator.get('input_channels', 3)} -> {generator.get('output_channels', 4)}\")\n",
    "        print(f\"    Base features: {ngf}\")\n",
    "        print(f\"    Residual blocks: {n_blocks}\")\n",
    "        print(f\"    Normalization: {generator.get('norm_layer', 'instance')}\")\n",
    "        print(f\"    Dropout: {generator.get('dropout', 0.3)}\")\n",
    "        \n",
    "        # Special features\n",
    "        if generator.get('use_attention', False):\n",
    "            print(f\"    Features: Self-attention mechanism\")\n",
    "        if generator.get('transformer_layers', 0) > 0:\n",
    "            print(f\"    Features: {generator.get('transformer_layers')} transformer layers\")\n",
    "            print(f\"    Features: {generator.get('attention_heads')} attention heads\")\n",
    "        \n",
    "        print(f\"  Discriminator:\")\n",
    "        print(f\"    Input channels: {discriminator.get('input_channels', 7)} (artwork+sprite)\")\n",
    "        print(f\"    Base features: {ndf}\")\n",
    "        print(f\"    Layers: {d_layers}\")\n",
    "        if discriminator.get('use_spectral_norm', False):\n",
    "            print(f\"    Features: Spectral normalization\")\n",
    "        \n",
    "        print(f\"  Training:\")\n",
    "        print(f\"    Image size: {params.get('image_size', 256)}px\")\n",
    "        print(f\"    L1 loss weight: {params.get('lambda_l1', 150)}\")\n",
    "        if params.get('lambda_perceptual', 0) > 0:\n",
    "            print(f\"    Perceptual loss: {params.get('lambda_perceptual')}\")\n",
    "        if params.get('lambda_pixel_art', 0) > 0:\n",
    "            print(f\"    Pixel art loss: {params.get('lambda_pixel_art')}\")\n",
    "        \n",
    "        print(f\"  Estimated parameters: ~{total_params:.1f}M\")\n",
    "        \n",
    "        # Task-specific assessment\n",
    "        suitability_score = 0\n",
    "        strengths = []\n",
    "        considerations = []\n",
    "        \n",
    "        # Model-specific analysis\n",
    "        if 'lightweight' in model_name:\n",
    "            strengths.append(\"Fast training and inference\")\n",
    "            strengths.append(\"Low memory requirements\")\n",
    "            strengths.append(\"Reduced overfitting risk\")\n",
    "            considerations.append(\"May lack capacity for complex mappings\")\n",
    "            suitability_score = 7  # Good for baseline\n",
    "            \n",
    "        elif 'sprite-optimized' in model_name:\n",
    "            strengths.append(\"Attention mechanism for detail preservation\")\n",
    "            strengths.append(\"Optimized loss weights for pixel art\")\n",
    "            strengths.append(\"Spectral normalization for stability\")\n",
    "            strengths.append(\"RGBA support with proper channel handling\")\n",
    "            considerations.append(\"Balanced complexity for dataset size\")\n",
    "            suitability_score = 9  # Recommended primary model\n",
    "            \n",
    "        elif 'transformer' in model_name:\n",
    "            strengths.append(\"Long-range dependency modeling\")\n",
    "            strengths.append(\"Advanced attention mechanisms\")\n",
    "            strengths.append(\"State-of-the-art architecture\")\n",
    "            considerations.append(\"Higher computational requirements\")\n",
    "            considerations.append(\"May need careful regularization\")\n",
    "            suitability_score = 8  # Experimental but promising\n",
    "        \n",
    "        # Common advantages for all models\n",
    "        if params.get('image_size', 0) == 256:\n",
    "            strengths.append(\"Direct 256px compatibility\")\n",
    "            suitability_score += 1\n",
    "        \n",
    "        if generator.get('output_channels', 3) == 4:\n",
    "            strengths.append(\"RGBA transparency support\")\n",
    "            suitability_score += 1\n",
    "        \n",
    "        if params.get('lambda_l1', 0) >= 150:\n",
    "            strengths.append(\"Strong pixel-level accuracy emphasis\")\n",
    "            suitability_score += 1\n",
    "        \n",
    "        print(f\"  Strengths:\")\n",
    "        for strength in strengths:\n",
    "            print(f\"    + {strength}\")\n",
    "        \n",
    "        if considerations:\n",
    "            print(f\"  Considerations:\")\n",
    "            for consideration in considerations:\n",
    "                print(f\"    - {consideration}\")\n",
    "        \n",
    "        print(f\"  Suitability Score: {suitability_score}/12\")\n",
    "        architecture_analysis[model_name] = suitability_score\n",
    "    \n",
    "    # Implementation strategy\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(\"IMPLEMENTATION STRATEGY\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Sort models by score\n",
    "    sorted_models = sorted(architecture_analysis.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    print(f\"\\nRecommended Training Sequence:\")\n",
    "    for i, (model_name, score) in enumerate(sorted_models, 1):\n",
    "        role = \"\"\n",
    "        if 'lightweight' in model_name:\n",
    "            role = \"BASELINE - Quick validation and debugging\"\n",
    "        elif 'sprite-optimized' in model_name:\n",
    "            role = \"PRIMARY - Main production model\"\n",
    "        elif 'transformer' in model_name:\n",
    "            role = \"ADVANCED - Experimental state-of-the-art\"\n",
    "        \n",
    "        print(f\"  {i}. {model_name.replace('-', ' ').title()}\")\n",
    "        print(f\"     Role: {role}\")\n",
    "        print(f\"     Score: {score}/12\")\n",
    "        print(f\"     Use case: {'Start here for quick results' if i == 1 else 'Optimize after baseline' if i == 2 else 'Experiment if resources allow'}\")\n",
    "    \n",
    "    print(f\"\\nCurriculum Learning Compatibility:\")\n",
    "    print(f\"  All models support:\")\n",
    "    print(f\"    • Progressive input scaling (128px -> 192px -> 256px)\")\n",
    "    print(f\"    • Fixed 256px sprite output\")\n",
    "    print(f\"    • RGBA channel handling\")\n",
    "    print(f\"    • Instance normalization (better for style transfer)\")\n",
    "    print(f\"    • Optimized loss weights for pixel art\")\n",
    "    \n",
    "    print(f\"\\nNext Steps:\")\n",
    "    print(f\"  1. Start with lightweight-baseline for rapid prototyping\")\n",
    "    print(f\"  2. Validate data pipeline and training stability\")\n",
    "    print(f\"  3. Switch to sprite-optimized for production training\")\n",
    "    print(f\"  4. Experiment with transformer-enhanced if results plateau\")\n",
    "    print(f\"  5. Apply curriculum learning with progressively larger inputs\")\n",
    "    \n",
    "    return architecture_analysis\n",
    "\n",
    "# Analyze model architectures\n",
    "architecture_scores = analyze_model_architectures()\n",
    "\n",
    "print(f\"\\nSUMMARY:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Configuration cleaned up from 8+ models to 3 focused architectures:\")\n",
    "print(\"  • Removed outdated/suboptimal configurations\")\n",
    "print(\"  • Kept only models with 256px support and RGBA channels\")  \n",
    "print(\"  • Focused on curriculum learning compatibility\")\n",
    "print(\"  • Ready for immediate implementation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40555078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEARNING RATE OPTIMIZATION\n",
      "Using model training - no heuristics\n",
      "Device: cuda\n",
      "Using synthetic data for LR finding\n",
      "\n",
      "--- LR finder for: lightweight-baseline ---\n",
      "Device: cuda\n",
      "Using synthetic data for LR finding\n",
      "\n",
      "--- LR finder for: lightweight-baseline ---\n",
      "LR Range Test: 1.00e-07 → 1.00e+00 (30 iterations)\n",
      "LR Range Test: 1.00e-07 → 1.00e+00 (30 iterations)\n",
      "Iter 0: LR 1.00e-07, Loss 90.0913\n",
      "Iter 10: LR 2.15e-05, Loss 90.0890\n",
      "Iter 0: LR 1.00e-07, Loss 90.0913\n",
      "Iter 10: LR 2.15e-05, Loss 90.0890\n",
      "Iter 20: LR 4.64e-03, Loss 85.3171\n",
      "Early stop at iteration 23 - loss diverged\n",
      "Results: Optimal LR = 9.04e-04, Range = 9.04e-05 - 2.71e-03\n",
      "✓ Completed LR finding for lightweight-baseline\n",
      "\n",
      "--- LR finder for: sprite-optimized ---\n",
      "LR Range Test: 1.00e-07 → 1.00e+00 (30 iterations)\n",
      "Iter 20: LR 4.64e-03, Loss 85.3171\n",
      "Early stop at iteration 23 - loss diverged\n",
      "Results: Optimal LR = 9.04e-04, Range = 9.04e-05 - 2.71e-03\n",
      "✓ Completed LR finding for lightweight-baseline\n",
      "\n",
      "--- LR finder for: sprite-optimized ---\n",
      "LR Range Test: 1.00e-07 → 1.00e+00 (30 iterations)\n",
      "Iter 0: LR 1.00e-07, Loss 95.9543\n",
      "Iter 0: LR 1.00e-07, Loss 95.9543\n",
      "Iter 10: LR 2.15e-05, Loss 94.8046\n",
      "Iter 10: LR 2.15e-05, Loss 94.8046\n",
      "Early stop at iteration 20 - loss diverged\n",
      "Results: Optimal LR = 3.09e-04, Range = 3.09e-05 - 9.26e-04\n",
      "✓ Completed LR finding for sprite-optimized\n",
      "\n",
      "--- LR finder for: transformer-enhanced ---\n",
      "LR Range Test: 1.00e-07 → 1.00e+00 (30 iterations)\n",
      "Iter 0: LR 1.00e-07, Loss 94.6996\n",
      "Early stop at iteration 20 - loss diverged\n",
      "Results: Optimal LR = 3.09e-04, Range = 3.09e-05 - 9.26e-04\n",
      "✓ Completed LR finding for sprite-optimized\n",
      "\n",
      "--- LR finder for: transformer-enhanced ---\n",
      "LR Range Test: 1.00e-07 → 1.00e+00 (30 iterations)\n",
      "Iter 0: LR 1.00e-07, Loss 94.6996\n",
      "Iter 10: LR 2.15e-05, Loss 94.0820\n",
      "Iter 10: LR 2.15e-05, Loss 94.0820\n",
      "Early stop at iteration 19 - loss diverged\n",
      "Results: Optimal LR = 1.80e-04, Range = 1.80e-05 - 5.41e-04\n",
      "✓ Completed LR finding for transformer-enhanced\n",
      "\n",
      "Optimization complete for 3 models\n",
      "Early stop at iteration 19 - loss diverged\n",
      "Results: Optimal LR = 1.80e-04, Range = 1.80e-05 - 5.41e-04\n",
      "✓ Completed LR finding for transformer-enhanced\n",
      "\n",
      "Optimization complete for 3 models\n"
     ]
    }
   ],
   "source": [
    "# Import optimizers from the dedicated module\n",
    "import sys\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.append(str(src_path))\n",
    "\n",
    "import importlib\n",
    "if 'optimizers.lr_finder' in sys.modules:\n",
    "    importlib.reload(sys.modules['optimizers.lr_finder'])\n",
    "\n",
    "from optimizers.lr_finder import find_optimal_learning_rates\n",
    "\n",
    "# Run learning rate optimization using the optimizers module\n",
    "optimal_learning_rates = find_optimal_learning_rates(src_path / \"config\" / \"model_configs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6238f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH SIZE OPTIMIZATION\n",
      "Testing memory usage with models\n",
      "Device: cuda\n",
      "\n",
      "--- Testing batch sizes for: lightweight-baseline ---\n",
      "Testing memory usage and training speed...\n",
      "Batch Size | Memory (MB) | Time (ms) | Status\n",
      "--------------------------------------------------\n",
      "        1 |      906.9 |     47.2 | ✓ Success\n",
      "        2 |       69.1 |     12.1 | ✓ Success\n",
      "        1 |      906.9 |     47.2 | ✓ Success\n",
      "        2 |       69.1 |     12.1 | ✓ Success\n",
      "        4 |      171.4 |     21.4 | ✓ Success\n",
      "        8 |      377.3 |     26.2 | ✓ Success\n",
      "        4 |      171.4 |     21.4 | ✓ Success\n",
      "        8 |      377.3 |     26.2 | ✓ Success\n",
      "       16 |      786.3 |     66.3 | ✓ Success\n",
      "       16 |      786.3 |     66.3 | ✓ Success\n",
      "       32 |     1595.8 |     42.9 | ✓ Success\n",
      "       32 |     1595.8 |     42.9 | ✓ Success\n",
      "       64 |     3225.3 |     70.8 | ✓ Success\n",
      "\n",
      "Dynamic extension check: last_successful=64, max_tested=64\n",
      "\n",
      "Extending tests beyond 64 since max tested was successful...\n",
      "       64 |     3225.3 |     70.8 | ✓ Success\n",
      "\n",
      "Dynamic extension check: last_successful=64, max_tested=64\n",
      "\n",
      "Extending tests beyond 64 since max tested was successful...\n",
      "      128 |     5252.8 |    134.7 | ✓ Success (extended)\n",
      "      128 |     5252.8 |    134.7 | ✓ Success (extended)\n",
      "      256 |    10570.4 |    715.8 | ✓ Success (extended)\n",
      "\n",
      "Results:\n",
      "Max stable: 256\n",
      "Most efficient: 128\n",
      "Recommended: 192\n",
      "✓ Completed for lightweight-baseline\n",
      "\n",
      "--- Testing batch sizes for: sprite-optimized ---\n",
      "      256 |    10570.4 |    715.8 | ✓ Success (extended)\n",
      "\n",
      "Results:\n",
      "Max stable: 256\n",
      "Most efficient: 128\n",
      "Recommended: 192\n",
      "✓ Completed for lightweight-baseline\n",
      "\n",
      "--- Testing batch sizes for: sprite-optimized ---\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing memory usage and training speed...\n",
      "Batch Size | Memory (MB) | Time (ms) | Status\n",
      "--------------------------------------------------\n",
      "        1 |      625.1 |     72.1 | ✓ Success\n",
      "        1 |      625.1 |     72.1 | ✓ Success\n",
      "        2 |      123.2 |     32.0 | ✓ Success\n",
      "        2 |      123.2 |     32.0 | ✓ Success\n",
      "        4 |      126.7 |     21.4 | ✓ Success\n",
      "        4 |      126.7 |     21.4 | ✓ Success\n",
      "        8 |      691.0 |     37.7 | ✓ Success\n",
      "        8 |      691.0 |     37.7 | ✓ Success\n",
      "       16 |     1223.1 |     49.4 | ✓ Success\n",
      "       16 |     1223.1 |     49.4 | ✓ Success\n",
      "       32 |     2701.8 |     96.4 | ✓ Success\n",
      "       32 |     2701.8 |     96.4 | ✓ Success\n",
      "       64 |     5662.7 |    150.9 | ✓ Success\n",
      "\n",
      "Dynamic extension check: last_successful=64, max_tested=64\n",
      "\n",
      "Extending tests beyond 64 since max tested was successful...\n",
      "       64 |     5662.7 |    150.9 | ✓ Success\n",
      "\n",
      "Dynamic extension check: last_successful=64, max_tested=64\n",
      "\n",
      "Extending tests beyond 64 since max tested was successful...\n",
      "      128 |    10617.6 |    109.8 | ✓ Success (extended)\n",
      "      128 |    10617.6 |    109.8 | ✓ Success (extended)\n",
      "      256 |    21746.6 |  64231.6 | ✓ Success (extended)\n",
      "\n",
      "Results:\n",
      "Max stable: 256\n",
      "Most efficient: 128\n",
      "Recommended: 192\n",
      "✓ Completed for sprite-optimized\n",
      "\n",
      "--- Testing batch sizes for: transformer-enhanced ---\n",
      "      256 |    21746.6 |  64231.6 | ✓ Success (extended)\n",
      "\n",
      "Results:\n",
      "Max stable: 256\n",
      "Most efficient: 128\n",
      "Recommended: 192\n",
      "✓ Completed for sprite-optimized\n",
      "\n",
      "--- Testing batch sizes for: transformer-enhanced ---\n",
      "Testing memory usage and training speed...\n",
      "Batch Size | Memory (MB) | Time (ms) | Status\n",
      "--------------------------------------------------\n",
      "Testing memory usage and training speed...\n",
      "Batch Size | Memory (MB) | Time (ms) | Status\n",
      "--------------------------------------------------\n",
      "        1 |      868.0 |     35.8 | ✓ Success\n",
      "        2 |      101.2 |     22.0 | ✓ Success\n",
      "        1 |      868.0 |     35.8 | ✓ Success\n",
      "        2 |      101.2 |     22.0 | ✓ Success\n",
      "        4 |      149.3 |     21.0 | ✓ Success\n",
      "        8 |      701.9 |     20.9 | ✓ Success\n",
      "        4 |      149.3 |     21.0 | ✓ Success\n",
      "        8 |      701.9 |     20.9 | ✓ Success\n",
      "       16 |     1215.0 |     25.5 | ✓ Success\n",
      "       16 |     1215.0 |     25.5 | ✓ Success\n",
      "       32 |     2655.9 |     45.0 | ✓ Success\n",
      "       32 |     2655.9 |     45.0 | ✓ Success\n",
      "       64 |     5534.1 |     67.8 | ✓ Success\n",
      "\n",
      "Dynamic extension check: last_successful=64, max_tested=64\n",
      "\n",
      "Extending tests beyond 64 since max tested was successful...\n",
      "       64 |     5534.1 |     67.8 | ✓ Success\n",
      "\n",
      "Dynamic extension check: last_successful=64, max_tested=64\n",
      "\n",
      "Extending tests beyond 64 since max tested was successful...\n",
      "      128 |    10331.0 |    245.2 | ✓ Success (extended)\n",
      "      128 |    10331.0 |    245.2 | ✓ Success (extended)\n",
      "      256 |    21103.3 |  57291.8 | ✓ Success (extended)\n",
      "\n",
      "Results:\n",
      "Max stable: 256\n",
      "Most efficient: 64\n",
      "Recommended: 192\n",
      "✓ Completed for transformer-enhanced\n",
      "      256 |    21103.3 |  57291.8 | ✓ Success (extended)\n",
      "\n",
      "Results:\n",
      "Max stable: 256\n",
      "Most efficient: 64\n",
      "Recommended: 192\n",
      "✓ Completed for transformer-enhanced\n"
     ]
    }
   ],
   "source": [
    "# Import batch optimizer from the optimizers module  \n",
    "import importlib\n",
    "if 'optimizers.batch_optimizer' in sys.modules:\n",
    "    importlib.reload(sys.modules['optimizers.batch_optimizer'])\n",
    "from optimizers.batch_optimizer import optimize_batch_sizes\n",
    "\n",
    "# Run batch size optimization using the optimizers module\n",
    "batch_size_recommendations = optimize_batch_sizes(src_path / \"config\" / \"model_configs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e78df5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL VALIDATION\n",
      "Creating and testing models\n",
      "Device: cuda\n",
      "Validating 3 model configurations...\n",
      "\n",
      "Validating lightweight-baseline...\n",
      "  ✓ Generator created: 4,128,164 parameters\n",
      "  ✓ Discriminator created: 169,697 parameters\n",
      "  ✓ Models moved to cuda\n",
      "  ✓ Forward pass successful\n",
      "    Generator: torch.Size([2, 3, 256, 256]) → torch.Size([2, 4, 256, 256])\n",
      "    Discriminator: torch.Size([2, 3, 256, 256]) + torch.Size([2, 4, 256, 256]) → torch.Size([2, 1, 62, 62])\n",
      "  ✓ Backward pass successful\n",
      "    Generator loss: 88.1697\n",
      "    Discriminator loss: 0.6356\n",
      "  ✅ lightweight-baseline: VALID\n",
      "\n",
      "Validating sprite-optimized...\n",
      "  ✓ Generator created: 30,660,420 parameters\n",
      "  ✓ Discriminator created: 2,768,833 parameters\n",
      "  ✓ Models moved to cuda\n",
      "  ✓ Forward pass successful\n",
      "    Generator: torch.Size([2, 3, 256, 256]) → torch.Size([2, 4, 256, 256])\n",
      "    Discriminator: torch.Size([2, 3, 256, 256]) + torch.Size([2, 4, 256, 256]) → torch.Size([2, 1, 30, 30])\n",
      "  ✓ Models moved to cuda\n",
      "  ✓ Forward pass successful\n",
      "    Generator: torch.Size([2, 3, 256, 256]) → torch.Size([2, 4, 256, 256])\n",
      "    Discriminator: torch.Size([2, 3, 256, 256]) + torch.Size([2, 4, 256, 256]) → torch.Size([2, 1, 62, 62])\n",
      "  ✓ Backward pass successful\n",
      "    Generator loss: 88.1697\n",
      "    Discriminator loss: 0.6356\n",
      "  ✅ lightweight-baseline: VALID\n",
      "\n",
      "Validating sprite-optimized...\n",
      "  ✓ Generator created: 30,660,420 parameters\n",
      "  ✓ Discriminator created: 2,768,833 parameters\n",
      "  ✓ Models moved to cuda\n",
      "  ✓ Forward pass successful\n",
      "    Generator: torch.Size([2, 3, 256, 256]) → torch.Size([2, 4, 256, 256])\n",
      "    Discriminator: torch.Size([2, 3, 256, 256]) + torch.Size([2, 4, 256, 256]) → torch.Size([2, 1, 30, 30])\n",
      "  ✓ Backward pass successful\n",
      "  ✓ Backward pass successful\n",
      "    Generator loss: 93.1769\n",
      "    Discriminator loss: 1.0044\n",
      "  ✅ sprite-optimized: VALID\n",
      "\n",
      "Validating transformer-enhanced...\n",
      "  ✓ Generator created: 25,940,804 parameters\n",
      "  ✓ Discriminator created: 2,768,833 parameters\n",
      "  ✓ Models moved to cuda\n",
      "  ✓ Forward pass successful\n",
      "    Generator: torch.Size([2, 3, 256, 256]) → torch.Size([2, 4, 256, 256])\n",
      "    Discriminator: torch.Size([2, 3, 256, 256]) + torch.Size([2, 4, 256, 256]) → torch.Size([2, 1, 30, 30])\n",
      "  ✓ Backward pass successful\n",
      "    Generator loss: 93.1769\n",
      "    Discriminator loss: 1.0044\n",
      "  ✅ sprite-optimized: VALID\n",
      "\n",
      "Validating transformer-enhanced...\n",
      "  ✓ Generator created: 25,940,804 parameters\n",
      "  ✓ Discriminator created: 2,768,833 parameters\n",
      "  ✓ Models moved to cuda\n",
      "  ✓ Forward pass successful\n",
      "    Generator: torch.Size([2, 3, 256, 256]) → torch.Size([2, 4, 256, 256])\n",
      "    Discriminator: torch.Size([2, 3, 256, 256]) + torch.Size([2, 4, 256, 256]) → torch.Size([2, 1, 30, 30])\n",
      "  ✓ Backward pass successful\n",
      "    Generator loss: 91.8637\n",
      "    Discriminator loss: 0.6633\n",
      "  ✅ transformer-enhanced: VALID\n",
      "\n",
      "VALIDATION SUMMARY\n",
      "==================================================\n",
      "Valid models: 3\n",
      "  ✓ lightweight-baseline\n",
      "  ✓ sprite-optimized\n",
      "  ✓ transformer-enhanced\n",
      "\n",
      "Total parameters across all valid models: 66,436,751\n",
      "    Generator loss: 91.8637\n",
      "    Discriminator loss: 0.6633\n",
      "  ✅ transformer-enhanced: VALID\n",
      "\n",
      "VALIDATION SUMMARY\n",
      "==================================================\n",
      "Valid models: 3\n",
      "  ✓ lightweight-baseline\n",
      "  ✓ sprite-optimized\n",
      "  ✓ transformer-enhanced\n",
      "\n",
      "Total parameters across all valid models: 66,436,751\n"
     ]
    }
   ],
   "source": [
    "# Import model validator from the optimizers module\n",
    "import importlib\n",
    "if 'optimizers.model_validator' in sys.modules:\n",
    "    importlib.reload(sys.modules['optimizers.model_validator'])\n",
    "from optimizers.model_validator import optimize_model_config\n",
    "\n",
    "# Run model configuration validation using the optimizers module\n",
    "validation_results = optimize_model_config(src_path / \"config\" / \"model_configs.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e3b4a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating optimized training schedules and updating configuration...\n",
      "OPTIMAL TRAINING PLAN FOR POKEMON SPRITE GENERATION\n",
      "============================================================\n",
      "\n",
      "PLAN FOR: LIGHTWEIGHT BASELINE\n",
      "==================================================\n",
      "Creating Curriculum Learning Schedule (lightweight complexity)\n",
      "============================================================\n",
      "STAGE 1: Foundation Stage\n",
      "  Resolution: 128px → 256px\n",
      "  Epochs: 30\n",
      "  Batch size: 45\n",
      "  Learning rate: 2.00e-04\n",
      "  Augmentation: minimal\n",
      "  Focus: Basic shape and structure learning\n",
      "  Batches per epoch: 17\n",
      "\n",
      "STAGE 2: Refinement Stage\n",
      "  Resolution: 192px → 256px\n",
      "  Epochs: 25\n",
      "  Batch size: 24\n",
      "  Learning rate: 1.40e-04\n",
      "  Augmentation: moderate\n",
      "  Focus: Detail enhancement and color learning\n",
      "  Batches per epoch: 32\n",
      "\n",
      "STAGE 3: Polish Stage\n",
      "  Resolution: 256px → 256px\n",
      "  Epochs: 20\n",
      "  Batch size: 16\n",
      "  Learning rate: 8.00e-05\n",
      "  Augmentation: strong\n",
      "  Focus: Fine details and precise output\n",
      "  Batches per epoch: 48\n",
      "\n",
      "TOTAL TRAINING: 75 epochs across 3 stages\n",
      "EXECUTION ORDER:\n",
      "  1. Foundation Stage: 30 epochs @ 128px\n",
      "  2. Refinement Stage: 25 epochs @ 192px\n",
      "  3. Polish Stage: 20 epochs @ 256px\n",
      "\n",
      "RESOURCE REQUIREMENTS:\n",
      "  Estimated total training time: 75 epochs\n",
      "  GPU memory needed: 6GB+\n",
      "  Base learning rate: 2.00e-04\n",
      "\n",
      "PLAN FOR: SPRITE OPTIMIZED\n",
      "==================================================\n",
      "Creating Curriculum Learning Schedule (medium complexity)\n",
      "============================================================\n",
      "STAGE 1: Foundation Stage\n",
      "  Resolution: 128px → 256px\n",
      "  Epochs: 30\n",
      "  Batch size: 22\n",
      "  Learning rate: 1.40e-04\n",
      "  Augmentation: minimal\n",
      "  Focus: Basic shape and structure learning\n",
      "  Batches per epoch: 35\n",
      "\n",
      "STAGE 2: Refinement Stage\n",
      "  Resolution: 192px → 256px\n",
      "  Epochs: 25\n",
      "  Batch size: 12\n",
      "  Learning rate: 9.80e-05\n",
      "  Augmentation: moderate\n",
      "  Focus: Detail enhancement and color learning\n",
      "  Batches per epoch: 64\n",
      "\n",
      "STAGE 3: Polish Stage\n",
      "  Resolution: 256px → 256px\n",
      "  Epochs: 20\n",
      "  Batch size: 8\n",
      "  Learning rate: 5.60e-05\n",
      "  Augmentation: strong\n",
      "  Focus: Fine details and precise output\n",
      "  Batches per epoch: 96\n",
      "\n",
      "TOTAL TRAINING: 75 epochs across 3 stages\n",
      "EXECUTION ORDER:\n",
      "  1. Foundation Stage: 30 epochs @ 128px\n",
      "  2. Refinement Stage: 25 epochs @ 192px\n",
      "  3. Polish Stage: 20 epochs @ 256px\n",
      "\n",
      "RESOURCE REQUIREMENTS:\n",
      "  Estimated total training time: 75 epochs\n",
      "  GPU memory needed: 6GB+\n",
      "  Base learning rate: 1.40e-04\n",
      "\n",
      "PLAN FOR: TRANSFORMER ENHANCED\n",
      "==================================================\n",
      "Creating Curriculum Learning Schedule (heavy complexity)\n",
      "============================================================\n",
      "STAGE 1: Foundation Stage\n",
      "  Resolution: 128px → 256px\n",
      "  Epochs: 30\n",
      "  Batch size: 11\n",
      "  Learning rate: 1.00e-04\n",
      "  Augmentation: minimal\n",
      "  Focus: Basic shape and structure learning\n",
      "  Batches per epoch: 70\n",
      "\n",
      "STAGE 2: Refinement Stage\n",
      "  Resolution: 192px → 256px\n",
      "  Epochs: 25\n",
      "  Batch size: 6\n",
      "  Learning rate: 7.00e-05\n",
      "  Augmentation: moderate\n",
      "  Focus: Detail enhancement and color learning\n",
      "  Batches per epoch: 128\n",
      "\n",
      "STAGE 3: Polish Stage\n",
      "  Resolution: 256px → 256px\n",
      "  Epochs: 20\n",
      "  Batch size: 4\n",
      "  Learning rate: 4.00e-05\n",
      "  Augmentation: strong\n",
      "  Focus: Fine details and precise output\n",
      "  Batches per epoch: 191\n",
      "\n",
      "TOTAL TRAINING: 75 epochs across 3 stages\n",
      "EXECUTION ORDER:\n",
      "  1. Foundation Stage: 30 epochs @ 128px\n",
      "  2. Refinement Stage: 25 epochs @ 192px\n",
      "  3. Polish Stage: 20 epochs @ 256px\n",
      "\n",
      "RESOURCE REQUIREMENTS:\n",
      "  Estimated total training time: 75 epochs\n",
      "  GPU memory needed: 8GB+\n",
      "  Base learning rate: 1.00e-04\n",
      "\n",
      "============================================================\n",
      "UPDATING CONFIGURATION FILE\n",
      "============================================================\n",
      "[SUCCESS] Updated ../src/config/model_configs.json with optimized training schedules\n",
      "[INFO] Added training schedules for 3 models\n",
      "[SUCCESS] Verification: 3 optimized schedules saved\n",
      "\n",
      "============================================================\n",
      "FINAL RECOMMENDATIONS\n",
      "============================================================\n",
      "1. Start with lightweight-baseline for quick validation\n",
      "2. Use curriculum learning: 128px → 192px → 256px\n",
      "3. Monitor validation loss at each stage\n",
      "4. Save checkpoints after each curriculum stage\n",
      "5. Use mixed precision training (fp16) to save memory\n",
      "6. Enable gradient accumulation if batch sizes are too small\n",
      "\n",
      "Optimized schedules saved to config file:\n",
      "  • lightweight-baseline: 75 total epochs\n",
      "  • sprite-optimized: 75 total epochs\n",
      "  • transformer-enhanced: 75 total epochs\n",
      "\n",
      "Next Steps:\n",
      "- Use updated config file with optimized schedules\n",
      "- Implement train.py with curriculum learning support\n",
      "- Configure logging and checkpointing\n",
      "- Start with the foundation stage (128px inputs)\n",
      "\n",
      "============================================================\n",
      "CONFIGURATION UPDATE VERIFICATION\n",
      "============================================================\n",
      "[SUCCESS] Optimized training schedules added to config file\n",
      "[INFO] Found 3 optimized schedules\n",
      "\n",
      "LIGHTWEIGHT BASELINE:\n",
      "  Total epochs: 75\n",
      "  Base learning rate: 2.00e-04\n",
      "  GPU requirement: 6GB+\n",
      "  Curriculum stages: 3\n",
      "    Stage 1: Foundation Stage - 30 epochs @ 128px\n",
      "    Stage 2: Refinement Stage - 25 epochs @ 192px\n",
      "    Stage 3: Polish Stage - 20 epochs @ 256px\n",
      "\n",
      "SPRITE OPTIMIZED:\n",
      "  Total epochs: 75\n",
      "  Base learning rate: 1.40e-04\n",
      "  GPU requirement: 6GB+\n",
      "  Curriculum stages: 3\n",
      "    Stage 1: Foundation Stage - 30 epochs @ 128px\n",
      "    Stage 2: Refinement Stage - 25 epochs @ 192px\n",
      "    Stage 3: Polish Stage - 20 epochs @ 256px\n",
      "\n",
      "TRANSFORMER ENHANCED:\n",
      "  Total epochs: 75\n",
      "  Base learning rate: 1.00e-04\n",
      "  GPU requirement: 8GB+\n",
      "  Curriculum stages: 3\n",
      "    Stage 1: Foundation Stage - 30 epochs @ 128px\n",
      "    Stage 2: Refinement Stage - 25 epochs @ 192px\n",
      "    Stage 3: Polish Stage - 20 epochs @ 256px\n",
      "\n",
      "Optimized training configuration is now ready for use with train.py\n",
      "Use: python train.py --config <model-name> --schedule optimized\n"
     ]
    }
   ],
   "source": [
    "# Import training schedule optimizer from the optimizers module\n",
    "import importlib\n",
    "if 'optimizers.schedule_optimizer' in sys.modules:\n",
    "    importlib.reload(sys.modules['optimizers.schedule_optimizer'])\n",
    "from optimizers.schedule_optimizer import create_optimal_training_plan\n",
    "\n",
    "# Create comprehensive training plan and update config file\n",
    "print(\"Creating optimized training schedules and updating configuration...\")\n",
    "training_plans = create_optimal_training_plan(src_path / \"config\" / \"model_configs.json\")\n",
    "\n",
    "# Verify that the config file was updated\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONFIGURATION UPDATE VERIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load and display the updated config section\n",
    "with open(src_path / \"config\" / \"model_configs.json\", 'r') as f:\n",
    "    updated_config = json.load(f)\n",
    "\n",
    "if 'optimized_training_schedules' in updated_config:\n",
    "    print(\"[SUCCESS] Optimized training schedules added to config file\")\n",
    "    print(f\"[INFO] Found {len(updated_config['optimized_training_schedules'])} optimized schedules\")\n",
    "    \n",
    "    for model_name, schedule in updated_config['optimized_training_schedules'].items():\n",
    "        print(f\"\\n{model_name.upper().replace('-', ' ')}:\")\n",
    "        print(f\"  Total epochs: {schedule['total_epochs']}\")\n",
    "        print(f\"  Base learning rate: {schedule['base_learning_rate']:.2e}\")\n",
    "        print(f\"  GPU requirement: {schedule['gpu_memory_requirement']}\")\n",
    "        print(f\"  Curriculum stages: {len(schedule['stages'])}\")\n",
    "        \n",
    "        for i, stage in enumerate(schedule['stages'], 1):\n",
    "            print(f\"    Stage {i}: {stage['stage_name']} - {stage['epochs']} epochs @ {stage['input_resolution']}px\")\n",
    "else:\n",
    "    print(\"[FAIL] Optimized training schedules not found in config file\")\n",
    "\n",
    "print(f\"\\nOptimized training configuration is now ready for use with train.py\")\n",
    "print(f\"Use: python train.py --config <model-name> --schedule optimized\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
